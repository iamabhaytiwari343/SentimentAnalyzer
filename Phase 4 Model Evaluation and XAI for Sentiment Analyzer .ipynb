{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86584429",
   "metadata": {},
   "source": [
    "## Importing all dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d0f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6d02b",
   "metadata": {},
   "source": [
    "## Loading of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21565e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = pd.read_csv('Dataset\\dataset1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7142c7",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize stopword as per data\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "new_stopwords = [\"would\",\"shall\",\"could\",\"might\"]\n",
    "stop_words.extend(new_stopwords)\n",
    "stop_words.remove(\"not\")\n",
    "stop_words=set(stop_words)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------Data Cleaning and Preprocessing pipeline----------------------------------'''\n",
    "\n",
    "#Removing special character\n",
    "def remove_special_character(content):\n",
    "    return re.sub('\\W+',' ', content )#re.sub('\\[[^&@#!]]*\\]', '', content)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_url(content):\n",
    "    return re.sub(r'http\\S+', '', content)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(content):\n",
    "    clean_data = []\n",
    "    for i in content.split():\n",
    "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
    "            clean_data.append(i.strip().lower())\n",
    "    return \" \".join(clean_data)\n",
    "\n",
    "# Expansion of english contractions\n",
    "def contraction_expansion(content):\n",
    "    content = re.sub(r\"won\\'t\", \"would not\", content)\n",
    "    content = re.sub(r\"can\\'t\", \"can not\", content)\n",
    "    content = re.sub(r\"don\\'t\", \"do not\", content)\n",
    "    content = re.sub(r\"shouldn\\'t\", \"should not\", content)\n",
    "    content = re.sub(r\"needn\\'t\", \"need not\", content)\n",
    "    content = re.sub(r\"hasn\\'t\", \"has not\", content)\n",
    "    content = re.sub(r\"haven\\'t\", \"have not\", content)\n",
    "    content = re.sub(r\"weren\\'t\", \"were not\", content)\n",
    "    content = re.sub(r\"mightn\\'t\", \"might not\", content)\n",
    "    content = re.sub(r\"didn\\'t\", \"did not\", content)\n",
    "    content = re.sub(r\"n\\'t\", \" not\", content)\n",
    "    '''content = re.sub(r\"\\'re\", \" are\", content)\n",
    "    content = re.sub(r\"\\'s\", \" is\", content)\n",
    "    content = re.sub(r\"\\'d\", \" would\", content)\n",
    "    content = re.sub(r\"\\'ll\", \" will\", content)\n",
    "    content = re.sub(r\"\\'t\", \" not\", content)\n",
    "    content = re.sub(r\"\\'ve\", \" have\", content)\n",
    "    content = re.sub(r\"\\'m\", \" am\", content)'''\n",
    "    return content\n",
    "\n",
    "#Data preprocessing\n",
    "def data_cleaning(content):\n",
    "    content = contraction_expansion(content)\n",
    "    content = remove_special_character(content)\n",
    "    content = remove_url(content)\n",
    "    \n",
    "    content = remove_stopwords(content)    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pd.options.display.max_colwidth = 1000\n",
    "#Data cleaning\n",
    "df['Reviews_clean']=df['Reviews'].apply(data_cleaning)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22792515",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping rating data to Binary label 1 (+ve) if rating >=7 and 0 (-ve) if rating <=4 and 2 (neutral) if rating = 5 or 6\n",
    "df['Label'] = df['Ratings'].apply(lambda x: '1' if x >= 7 else ('0' if x<=4 else '2'))\n",
    "#Removing \n",
    "df=df[df.Label<'2']\n",
    "data=df[['Reviews_clean','Reviews','Ratings','Label']]\n",
    "print(data['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad126e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dependencies for feature engineering \n",
    "import sys\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "from prettytable import PrettyTable\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755bcab",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bde6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of word \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wordnetlemma = WordNetLemmatizer()\n",
    "    def __call__(self, reviews):\n",
    "        return [self.wordnetlemma.lemmatize(word) for word in word_tokenize(reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d38c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test=train_test_split(data,test_size=.3,random_state=42, shuffle=True)\n",
    "#countvect = CountVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3), min_df=10,max_features=5000)\n",
    "tfidfvect = TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)\n",
    "#x_train_count = countvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "#x_test_count = countvect.transform(test['Reviews_clean']).toarray()\n",
    "x_train_tfidf = tfidfvect.fit_transform(train['Reviews_clean']).toarray()\n",
    "x_test_tfidf = tfidfvect.transform(test['Reviews_clean']).toarray()\n",
    "\n",
    "y_train = train['Label']\n",
    "y_test = test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list=y_test.tolist()\n",
    "# y_predict_list=y_predict.tolist()\n",
    "test_list=test['Reviews_clean'].tolist()\n",
    "rating_list=test['Ratings'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633921c3",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ed43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prerequisite libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score, accuracy_score,precision_recall_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b534b",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_1 = Pipeline(\n",
    "    steps=[\n",
    "        #best base model(\"classifier\", LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=1.0, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None)),\n",
    "    (\"classifier\", LogisticRegression())]\n",
    ")'''\n",
    "model_1=LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=10, solver='lbfgs', max_iter=200, multi_class='auto', verbose=0, warm_start=False, n_jobs=None))\n",
    "model_2=Pipeline(\n",
    "    steps=[\n",
    "        #best base model(\"classifier\", LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=1.0, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None)),\n",
    "    ('vect',TfidfVectorizer(analyzer = \"word\", tokenizer = LemmaTokenizer(), ngram_range=(1,3),min_df=10,max_features=10000)),(\"classifier\", LogisticRegression(penalty='l2',dual=False, tol=0.0001, C=10, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977f08d",
   "metadata": {},
   "source": [
    "## Training of Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6058e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_1.fit(x_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_2.fit(train['Reviews_clean'],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f84562",
   "metadata": {},
   "source": [
    "## Evaluation on multiple metrics dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cdfb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Precision Score for Logistic Regression: %s\" % precision_score(y_test,model_1.predict(x_test_tfidf),average='micro'))\n",
    "print(\"Recall Score for Logistic Regression: %s\" % recall_score(y_test,model_1.predict(x_test_tfidf),average='micro'))\n",
    "print(\"AUC Score for Logistic Regression: %s\" % roc_auc_score(y_test,model_1.predict_proba(x_test_tfidf)[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_1 =f1_score(y_test,model_1.predict(x_test_tfidf),average=\"weighted\")\n",
    "print(\"F1 Score for Logistic Regression: %s\" % f1_score_1)\n",
    "print(\"Accuracy Score for Logistic Regression: %s\" % accuracy_score(y_test,model_1.predict(x_test_tfidf)))\n",
    "print(\"Precision Score for Logistic Regression Pipeline: %s\" % precision_score(y_test,model_2.predict(test['Reviews_clean']),average='micro'))\n",
    "print(\"Recall Score for Logistic Regression Pipeline: %s\" % recall_score(y_test,model_2.predict(test['Reviews_clean']),average='micro'))\n",
    "print(\"AUC Score for Logistic Regression Pipeline: %s\" % roc_auc_score(y_test,model_2.predict_proba(test['Reviews_clean'])[:,1],multi_class='ovo',average='macro'))\n",
    "f1_score_2 =f1_score(y_test,model_2.predict(test['Reviews_clean']),average=\"weighted\")\n",
    "print(\"F1 Score for Logistic Regression Pipeline: %s\" % f1_score_2)\n",
    "print(\"Accuracy Score for Logistic Regression Pipeline: %s\" % accuracy_score(y_test,model_2.predict(test['Reviews_clean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model_1.predict(x_test_tfidf)\n",
    "y_predict_prob=model_1.predict_proba(x_test_tfidf)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce8725d",
   "metadata": {},
   "source": [
    "## Confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02145792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_plot(y_test,y_score):\n",
    "    confmatrix = confusion_matrix(y_test,y_score)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(confmatrix)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, confmatrix[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_plot(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9b26be",
   "metadata": {},
   "source": [
    "## Analyzing False Positive and False Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from colorama import Fore, Back, Style\n",
    "fn_dict={}\n",
    "fp_dict={}\n",
    "for i in range(0, len(y_test_list)):\n",
    "    if ((y_test_list[i]=='0') & (y_predict_list[i]=='1')):\n",
    "        fp_dict[i]=[test_list[i],rating_list[i]]\n",
    "    elif((y_test_list[i]=='1') & (y_predict_list[i]=='0')):\n",
    "        fn_dict[i]=[test_list[i],rating_list[i]]\n",
    "    else:\n",
    "        pass\n",
    "    i+=1\n",
    "for k,v in fp_dict.items():\n",
    "    if v[1]<=2:\n",
    "        print(Fore.RED +'False Positive: %s %s'%(k,v))\n",
    "for k,v in fn_dict.items():\n",
    "    if v[1]>=9:\n",
    "        print(Fore.GREEN +'False Negative: %s %s'%(k,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cf728e",
   "metadata": {},
   "source": [
    "# XAI: Explainable AI by Shap and LIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ac6e4",
   "metadata": {},
   "source": [
    "## Explain Marginal Contribution of Features by Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e7fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aadc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model_1, x_train_tfidf, feature_names=tfidfvect.get_feature_names())\n",
    "shap_values = explainer(x_test_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3635133",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99db745",
   "metadata": {},
   "source": [
    "## Visualizing Marginal Contribution of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67286da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 4443\n",
    "print('Probability Score %s' %y_predict_prob[ind])\n",
    "shap.plots.force(shap_values[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52d04d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list=y_test.tolist()\n",
    "print(\"Positive\" if y_test_list[ind] else \"Negative\", \"Review:\")\n",
    "print(test_list[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161b5597",
   "metadata": {},
   "source": [
    "## Visualizing Marginal Contribution of Features for False Positive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc7760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 111\n",
    "print('Probability Score %s' %y_predict_prob[ind])\n",
    "shap.plots.force(shap_values[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4201ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list=y_test.tolist()\n",
    "print(\"Positive\" if y_test_list[ind] else \"Negative\", \"Review:\")\n",
    "print(test_list[ind])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2622cd4",
   "metadata": {},
   "source": [
    "## Visualizing Marginal Contribution of Features for False Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 7599\n",
    "print('Probability Score %s' %y_predict_prob[ind])\n",
    "shap.plots.force(shap_values[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b84f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_list=y_test.tolist()\n",
    "print(\"Positive\" if y_test_list[ind] else \"Negative\", \"Review:\")\n",
    "print(test_list[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213490d",
   "metadata": {},
   "source": [
    "## Explain feature impact on Prediction by LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5594e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=7599\n",
    "output = model_2.predict([test_list[idx]])\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5862aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names = [0,1]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "exp = explainer.explain_instance(test_list[idx], model_2.predict_proba, num_features = 100,top_labels=2)\n",
    "print('New document id: %d' % idx)\n",
    "print('Predicted Label =', model_2.predict([test_list[idx]]))\n",
    "print('Predicted probabilites =', model_2.predict_proba([test_list[idx]]))\n",
    "print('Actual Label: %s' % y_test_list[idx])\n",
    "print(exp.available_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b865b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b958a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list(label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85af41b",
   "metadata": {},
   "source": [
    "## Explain feature impact on False Positive by LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65272ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=111\n",
    "exp = explainer.explain_instance(test_list[idx], model_2.predict_proba, num_features = 100,top_labels=2)\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec88d9b",
   "metadata": {},
   "source": [
    "## Explain feature impact on False Positive by LIME¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eb22c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=24097\n",
    "exp = explainer.explain_instance(test_list[idx], model_2.predict_proba, num_features = 100,top_labels=2)\n",
    "exp.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb1bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=input()\n",
    "exp = explainer.explain_instance(test, model_2.predict_proba, num_features = 100,top_labels=2)\n",
    "exp.show_in_notebook(text=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
